<source-viewer _ngcontent-ng-c3140449333="" class="ng-tns-c3140449333-5" _nghost-ng-c2419719638="">
    <div _ngcontent-ng-c2419719638="" class="background" jslog="202049;track:impression">
        <div _ngcontent-ng-c2419719638="" class="fixed-container">
            <div _ngcontent-ng-c2419719638="" class="header">
                <div _ngcontent-ng-c2419719638="" class="header-column">
                    <div _ngcontent-ng-c2419719638="" class="source-title-container"><!----><a
                            _ngcontent-ng-c2419719638="" class="source-title-link ng-star-inserted">
                            <div _ngcontent-ng-c2419719638="" id="source-title" class="source-title"
                                title="Can Whisper be used for real-time streaming ASR?">Can Whisper be used for
                                real-time streaming ASR?</div>
                        </a><button _ngcontent-ng-c2419719638="" mat-icon-button="" mattooltip="Open in new tab"
                            aria-describedby="source-title" aria-label="Open in new tab"
                            class="mdc-icon-button mat-mdc-icon-button mat-mdc-button-base mat-mdc-tooltip-trigger source-link-button mat-unthemed ng-star-inserted"
                            mat-ripple-loader-uninitialized="" mat-ripple-loader-class-name="mat-mdc-button-ripple"
                            mat-ripple-loader-centered="" jslog="248144;track:generic_click,impression"><span
                                class="mat-mdc-button-persistent-ripple mdc-icon-button__ripple"></span><mat-icon
                                _ngcontent-ng-c2419719638="" role="img" aria-hidden="true"
                                class="mat-icon notranslate google-symbols mat-icon-no-color"
                                data-mat-icon-type="font">open_in_new</mat-icon><span
                                class="mat-focus-indicator"></span><span
                                class="mat-mdc-button-touch-target"></span></button><!----><!----><!----></div>
                    <div _ngcontent-ng-c2419719638="" class="source-title-subtext mat-body-medium"><!----><!----></div>
                    <!---->
                </div>
            </div>
            <div _ngcontent-ng-c2419719638="" class="source-guide-container ng-star-inserted">
                <div _ngcontent-ng-c2419719638="" class="source-guide-heading">
                    <div _ngcontent-ng-c2419719638="" role="heading" aria-level="4"
                        class="source-guide-label mat-title-medium"><mat-icon _ngcontent-ng-c2419719638="" role="img"
                            class="mat-icon notranslate source-guide-label__sparkle mat-icon-rtl-mirror google-symbols mat-icon-no-color"
                            aria-hidden="true" data-mat-icon-type="font">button_magic</mat-icon> Source guide </div>
                    <div _ngcontent-ng-c2419719638="" class="source-guide-spinner-container"><!----></div><button
                        _ngcontent-ng-c2419719638="" mat-icon-button="" color="primary"
                        class="mdc-icon-button mat-mdc-icon-button mat-mdc-button-base mat-mdc-tooltip-trigger open-source-guide-button mat-primary"
                        mat-ripple-loader-uninitialized="" mat-ripple-loader-class-name="mat-mdc-button-ripple"
                        mat-ripple-loader-centered="" aria-label="Close source guide" aria-expanded="true"
                        jslog="189030;track:generic_click,impression"><span
                            class="mat-mdc-button-persistent-ripple mdc-icon-button__ripple"></span><mat-icon
                            _ngcontent-ng-c2419719638="" role="img"
                            class="mat-icon notranslate icon google-symbols mat-icon-no-color" aria-hidden="true"
                            data-mat-icon-type="font">arrow_drop_up</mat-icon><span
                            class="mat-focus-indicator"></span><span
                            class="mat-mdc-button-touch-target"></span></button><!---->
                </div>
                <div _ngcontent-ng-c2419719638="" class="source-guide-rows ng-star-inserted"
                    jslog="189021;track:impression">
                    <div _ngcontent-ng-c2419719638="" class="summary-container"><!---->
                        <div _ngcontent-ng-c2419719638="" class="summary ng-star-inserted">
                            <div _ngcontent-ng-c2419719638="" class="mat-body-medium ng-star-inserted">
                                <p>While OpenAI’s Whisper model was originally designed for processing audio in large
                                    batches, it can be adapted for <strong>real-time streaming</strong> through a
                                    specialized buffering technique. This approach works by feeding
                                    <strong>progressively larger chunks of audio</strong> into the system until the
                                    model identifies a completed sentence, at which point the buffer resets. To ensure
                                    accuracy during this live process, a <strong>local agreement algorithm</strong> is
                                    used, where text is only confirmed and displayed permanently once it consistently
                                    appears in multiple consecutive processing cycles. Although this method is less
                                    efficient than models built specifically for streaming—because it
                                    <strong>re-processes the start of sentences</strong> multiple times—it successfully
                                    transforms Whisper into a responsive tool for live transcription and AI writing
                                    assistants.</p>
                            </div><!---->
                        </div><!----><!---->
                    </div>
                    <div _ngcontent-ng-c2419719638="" class="key-topics-container ng-star-inserted"><mat-chip-listbox
                            _ngcontent-ng-c2419719638="" hidesingleselectionindicator="true"
                            class="mdc-evolution-chip-set mat-mdc-chip-listbox mat-mdc-chip-set key-topics-chip-list"
                            role="listbox" tabindex="0" aria-required="false" aria-disabled="false"
                            aria-multiselectable="false" aria-orientation="horizontal">
                            <div role="presentation" class="mdc-evolution-chip-set__chips"><mat-chip-option
                                    _ngcontent-ng-c2419719638="" appearance="hairline-assistive" color="primary"
                                    class="mat-mdc-chip mat-mdc-chip-option key-topics-chip mat-primary mdc-evolution-chip mat-mdc-standard-chip mdc-evolution-chip--filter mdc-evolution-chip--selectable mdc-evolution-chip--selecting ng-star-inserted"
                                    mat-ripple-loader-uninitialized=""
                                    mat-ripple-loader-class-name="mat-mdc-chip-ripple" title="Whisper model"
                                    jslog="189023;track:generic_click,impression" id="mat-mdc-chip-25"
                                    role="presentation"><span class="mat-mdc-chip-focus-overlay"></span><span
                                        class="mdc-evolution-chip__cell mdc-evolution-chip__cell--primary"><button
                                            matchipaction="" role="option"
                                            class="mat-mdc-chip-action mdc-evolution-chip__action mdc-evolution-chip__action--primary"
                                            type="button" aria-selected="false" aria-disabled="false"
                                            tabindex="-1"><!----><span
                                                class="mdc-evolution-chip__text-label mat-mdc-chip-action-label"><span
                                                    _ngcontent-ng-c2419719638="" class="key-topics-text">
                                                    <p>Whisper model</p>
                                                </span><span
                                                    class="mat-mdc-chip-primary-focus-indicator mat-focus-indicator"></span></span></button></span><!----></mat-chip-option><mat-chip-option
                                    _ngcontent-ng-c2419719638="" appearance="hairline-assistive" color="primary"
                                    class="mat-mdc-chip mat-mdc-chip-option key-topics-chip mat-primary mdc-evolution-chip mat-mdc-standard-chip mdc-evolution-chip--filter mdc-evolution-chip--selectable mdc-evolution-chip--selecting ng-star-inserted"
                                    mat-ripple-loader-uninitialized=""
                                    mat-ripple-loader-class-name="mat-mdc-chip-ripple" title="Streaming ASR"
                                    jslog="189023;track:generic_click,impression" id="mat-mdc-chip-26"
                                    role="presentation"><span class="mat-mdc-chip-focus-overlay"></span><span
                                        class="mdc-evolution-chip__cell mdc-evolution-chip__cell--primary"><button
                                            matchipaction="" role="option"
                                            class="mat-mdc-chip-action mdc-evolution-chip__action mdc-evolution-chip__action--primary"
                                            type="button" aria-selected="false" aria-disabled="false"
                                            tabindex="-1"><!----><span
                                                class="mdc-evolution-chip__text-label mat-mdc-chip-action-label"><span
                                                    _ngcontent-ng-c2419719638="" class="key-topics-text">
                                                    <p>Streaming ASR</p>
                                                </span><span
                                                    class="mat-mdc-chip-primary-focus-indicator mat-focus-indicator"></span></span></button></span><!----></mat-chip-option><mat-chip-option
                                    _ngcontent-ng-c2419719638="" appearance="hairline-assistive" color="primary"
                                    class="mat-mdc-chip mat-mdc-chip-option key-topics-chip mat-primary mdc-evolution-chip mat-mdc-standard-chip mdc-evolution-chip--filter mdc-evolution-chip--selectable mdc-evolution-chip--selecting ng-star-inserted"
                                    mat-ripple-loader-uninitialized=""
                                    mat-ripple-loader-class-name="mat-mdc-chip-ripple" title="Speech recognition"
                                    jslog="189023;track:generic_click,impression" id="mat-mdc-chip-27"
                                    role="presentation"><span class="mat-mdc-chip-focus-overlay"></span><span
                                        class="mdc-evolution-chip__cell mdc-evolution-chip__cell--primary"><button
                                            matchipaction="" role="option"
                                            class="mat-mdc-chip-action mdc-evolution-chip__action mdc-evolution-chip__action--primary"
                                            type="button" aria-selected="false" aria-disabled="false"
                                            tabindex="-1"><!----><span
                                                class="mdc-evolution-chip__text-label mat-mdc-chip-action-label"><span
                                                    _ngcontent-ng-c2419719638="" class="key-topics-text">
                                                    <p>Speech recognition</p>
                                                </span><span
                                                    class="mat-mdc-chip-primary-focus-indicator mat-focus-indicator"></span></span></button></span><!----></mat-chip-option><mat-chip-option
                                    _ngcontent-ng-c2419719638="" appearance="hairline-assistive" color="primary"
                                    class="mat-mdc-chip mat-mdc-chip-option key-topics-chip mat-primary mdc-evolution-chip mat-mdc-standard-chip mdc-evolution-chip--filter mdc-evolution-chip--selectable mdc-evolution-chip--selecting ng-star-inserted"
                                    mat-ripple-loader-uninitialized=""
                                    mat-ripple-loader-class-name="mat-mdc-chip-ripple" title="Local agreement algorithm"
                                    jslog="189023;track:generic_click,impression" id="mat-mdc-chip-28"
                                    role="presentation"><span class="mat-mdc-chip-focus-overlay"></span><span
                                        class="mdc-evolution-chip__cell mdc-evolution-chip__cell--primary"><button
                                            matchipaction="" role="option"
                                            class="mat-mdc-chip-action mdc-evolution-chip__action mdc-evolution-chip__action--primary"
                                            type="button" aria-selected="false" aria-disabled="false"
                                            tabindex="-1"><!----><span
                                                class="mdc-evolution-chip__text-label mat-mdc-chip-action-label"><span
                                                    _ngcontent-ng-c2419719638="" class="key-topics-text">
                                                    <p>Local agreement algorithm</p>
                                                </span><span
                                                    class="mat-mdc-chip-primary-focus-indicator mat-focus-indicator"></span></span></button></span><!----></mat-chip-option><mat-chip-option
                                    _ngcontent-ng-c2419719638="" appearance="hairline-assistive" color="primary"
                                    class="mat-mdc-chip mat-mdc-chip-option key-topics-chip mat-primary mdc-evolution-chip mat-mdc-standard-chip mdc-evolution-chip--filter mdc-evolution-chip--selectable mdc-evolution-chip--selecting ng-star-inserted"
                                    mat-ripple-loader-uninitialized=""
                                    mat-ripple-loader-class-name="mat-mdc-chip-ripple" title="Real-time transcription"
                                    jslog="189023;track:generic_click,impression" id="mat-mdc-chip-29"
                                    role="presentation"><span class="mat-mdc-chip-focus-overlay"></span><span
                                        class="mdc-evolution-chip__cell mdc-evolution-chip__cell--primary"><button
                                            matchipaction="" role="option"
                                            class="mat-mdc-chip-action mdc-evolution-chip__action mdc-evolution-chip__action--primary"
                                            type="button" aria-selected="false" aria-disabled="false"
                                            tabindex="-1"><!----><span
                                                class="mdc-evolution-chip__text-label mat-mdc-chip-action-label"><span
                                                    _ngcontent-ng-c2419719638="" class="key-topics-text">
                                                    <p>Real-time transcription</p>
                                                </span><span
                                                    class="mat-mdc-chip-primary-focus-indicator mat-focus-indicator"></span></span></button></span><!----></mat-chip-option><!---->
                            </div>
                        </mat-chip-listbox></div><!---->
                </div><!---->
            </div><!---->
        </div>
        <div _ngcontent-ng-c2419719638="" class="scroll-container">
            <div _ngcontent-ng-c2419719638="" class="scroll-area ng-star-inserted">
                <div _ngcontent-ng-c2419719638="" class="youtube-container mat-body-medium ng-star-inserted">
                    <div _ngcontent-ng-c2419719638=""><iframe _ngcontent-ng-c2419719638=""
                            referrerpolicy="strict-origin-when-cross-origin" scrolling="no" style="border: 0;"
                            src="https://www.youtube-nocookie.com/embed/_spinzpEeFM?loop=1&amp;playlist=_spinzpEeFM"></iframe>
                    </div>
                </div><!---->
                <div _ngcontent-ng-c2419719638="" class="elements-container"><labs-tailwind-doc-viewer
                        _ngcontent-ng-c2419719638="" _nghost-ng-c266593781=""
                        class="ng-star-inserted"><!----><labs-tailwind-structural-element-view-v2
                            _ngcontent-ng-c266593781="" _nghost-ng-c632281813=""
                            class="ng-star-inserted"><!----><!----><!----><!---->
                            <div _ngcontent-ng-c632281813="" class="paragraph normal ng-star-inserted"
                                data-start-index="0"><!----><!----><!----><!----><!----><!----><!----><!----><span
                                    _ngcontent-ng-c632281813="" data-start-index="0" class="ng-star-inserted">can the
                                    open AI whisper model do streaming ASR my name is Bai i'm a machine learning
                                    engineer and a PhD in natural language processing and today I will answer this
                                    question so if you haven't seen the whisper model it is a speechtoext model it is
                                    trained on about 100 languages on 680,000 hours of data the model architecture is a
                                    encoder decoder transformer model and it comes in five different sizes and it is
                                    growing in popularity recently because it is robust to noise and accents so the
                                    question of the day is can this model do streaming speech recognition so first of
                                    all what do we mean by streaming ASR well automated speech recognition can come in
                                    two forms batch and streaming batch means that the model needs to take a bunch of
                                    speech and produce a bunch of text but streaming ASR is when the model needs to
                                    produce an output as the speaker is saying things with a delay of not more than a
                                    few seconds so for example if you're listening to a live broadcast you don't want to
                                    first record the entire broadcast and then produce the subtitles you want it to
                                    appear at most a few seconds after it is said and generally you'll expect streaming
                                    ASR to be a few percentage points lower accuracy than batch ASR because the model
                                    does not have access to all the context only the point up until the current word the
                                    question of how to do streaming ASR came up when I was building the voice writer
                                    this is an AI writing assistant that works in two steps in the first step you just
                                    speak your thoughts without worrying too much about the grammar and in the second
                                    step the AI corrects the grammar for you i've been finding it super helpful for
                                    writing all kinds of things including emails blog posts and Slack messages you can
                                    try it out for free in the link here and I'll also post a link in the description of
                                    the video now back to the video first of all why is this even difficult why is it
                                    not so easy to use the Whisper model for streaming ASR the issue is Whisper is
                                    trained to process input audio of 30 seconds long if the audio is less than 30
                                    seconds you can pad it to be longer so this is not too much of a problem but what if
                                    your audio is longer than 30 seconds there is no way to feed into Whisper anything
                                    longer than 30 seconds so the first thing you might think of is what if you just
                                    split it up into chunks of 30 seconds at a time and process each chunk one at a time
                                    well the first problem is you might split it in the middle of the word like if you
                                    split it while in the middle of the word this word will probably not be recognized
                                    correctly the second problem is the latency will be really high so imagine if you're
                                    processing audio you're waiting for audio to fill up these 30 seconds and only then
                                    will you process this entire chunk and then the latency um for for the first word
                                    will be up to 30 seconds fortunately there is an open source report called Whisper
                                    Streaming that does this for you and it turns the Whisper model into a streaming ASR
                                    system there are instructions on how to set this up so let's try it out so I run
                                    this command to start a model running um it's using a whisper small model and then I
                                    go to this tab and run another command and I just start speaking so you can pretty
                                    quickly see that it's um picking it up and it's pretty quick and responsive and it
                                    even gives timestamps so um it's pretty cool demo um now let's talk about how this
                                    works so we start with this audio file and what we do is we feed chunks of
                                    increasing size into whisper the size of these chunks is configured by the minimum
                                    chunk size parameter which by default is 1 second so in each iteration we increase
                                    the size of this buffer by 1 second and feed all of it into whisper and this process
                                    continues until we hit a end of sentence marker like a period or a question mark
                                    then we move the buffer forward and start the process again so each piece of audio
                                    is processed by whisper multiple times as many times as it takes until we hit the
                                    end of the sentence the reasons for this is whisper is trained on sentences so it
                                    gives the best result when the start of the audio aligns with the start of a
                                    sentence and that's why the buffer only moves forward when the previous sentence is
                                    complete and we're beginning a new sentence this way Whisper never needs to start
                                    transcribing from the middle of a sentence which would give suboptimal results in
                                    many applications it is useful to have some results available as soon as possible
                                    even if they're not completely accurate in the voice writer I am showing the
                                    incomplete results in gray whereas the confirmed results are in black the difference
                                    is the gray part of the sentence is unconfirmed so it may change as the model gets
                                    new information but the black part is the confirmed results and they are permanent
                                    how this works is using an algorithm called local agreement with n equals 2 this
                                    means that for a token to be confirmed it needs to be generated in two consecutive
                                    audio buffers let's give an example let's say in the first step the worst per model
                                    outputs the three tokens if you like and nothing is confirmed at this step in the
                                    second step the model produces more tokens but only the first two tokens agree with
                                    the previous step so those two are confirmed in the third and the fourth steps more
                                    tokens are generated but at each step it does not confirm any tokens until it is at
                                    least generated in two consecutive chunks so anything in the gray part is possible
                                    to change and given new information for example the word view might change to video
                                    once the model hears the rest of the sentence but anything in the black part is
                                    permanent so even if the model wants to change it to something else after more
                                    iterations it cannot be changed anymore one last thing that this model does when it
                                    generates a new sentence is it feeds the previous sentence into the model as prompt
                                    tokens so this is something that you can do in the whisper format is you can give it
                                    a bunch of prompt tokens before you start generating and this tends to improve the
                                    accuracy a little bit because more context is always good so in summary the
                                    algorithm can be summarized in three basic ideas the first idea is we feed longer
                                    and longer consecutive audio buffers into whisper and then we emit tokens as soon as
                                    they're confirmed by two iterations and finally we scroll forward the audio buffer
                                    whenever a sentence is completed so it's a pretty simple algorithm that you can
                                    apply to any speech to text model that does not support streaming and basically turn
                                    it into a streaming model if you like you can check out all the details in this
                                    paper which I'll link in the description one of the limitations comes from the fact
                                    that whisper was not really designed to be a streaming model and because of this it
                                    assumes that each audio buffer has to start at the beginning of a sentence therefore
                                    if you have a sentence that is quite long then the beginning of the sentence has to
                                    be fed into the model and processed many times this is inefficient and not necessary
                                    if the model was trained from the beginning to do streaming ASR now if we look at an
                                    architecture that was designed specifically to do streaming speech recognition it
                                    looks a little bit different here is a model that was proposed in 2021 at each step
                                    it predicts a token and it has access to a fixed amount of past context and future
                                    context during training this rule is enforced by having an intention mask that is
                                    mostly zeros meaning that the model cannot use information that is outside of this
                                    fixed window and during prediction the model predicts a token given a limited amount
                                    of fixed context to make the next prediction the entire receptive field moves
                                    forward by one chunk but the size of the receptive field is fixed this way the
                                    beginning of the sentence will not be processed multiple times however this is not
                                    possible with the whisper model because it requires modifying the architecture and
                                    retraining the model and we do not have access to whisper's training data that's it
                                    for this video and I hope you enjoyed my explanation of how Whisper can be turned
                                    into a streaming model if you enjoyed this video please leave a comment hit the
                                    subscribe button and ring the bell icon to stay notified when I release future
                                    videos goodbye </span><!----><!----><!----><!----><!----><!----><!----><!----></div>
                            <!----><!----><!----><!---->
                        </labs-tailwind-structural-element-view-v2><!----><!----></labs-tailwind-doc-viewer><!---->
                </div>
            </div><!---->
        </div>
    </div>
</source-viewer>